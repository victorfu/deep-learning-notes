# deep-learning-notes

## Papers

**Attention Is All You Need**. _Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin_. NIPS 2017. [[1706.03762](https://arxiv.org/abs/1706.03762)]

**BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**. _Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova_. NAACL 2019. [[1810.04805](https://arxiv.org/pdf/1810.04805.pdf)] [[code & model](https://github.com/google-research/bert)]

**Well-Read Students Learn Better: The Impact of Student Initialization on Knowledge Distillation**. _Iulia Turc, Ming-Wei Chang, Kenton Lee, Kristina Toutanova_. Preprint. [[1908.08962](https://arxiv.org/pdf/1908.08962.pdf)]
